{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1801ca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-22T20:42:27.628553Z",
     "iopub.status.busy": "2024-11-22T20:42:27.628022Z",
     "iopub.status.idle": "2024-11-22T20:42:27.635270Z",
     "shell.execute_reply": "2024-11-22T20:42:27.634711Z"
    },
    "papermill": {
     "duration": 0.015042,
     "end_time": "2024-11-22T20:42:27.636858",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.621816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Temporary Directory\n",
    "tmp = '/kaggle/tmp/'\n",
    "# os.mkdir(tmp)\n",
    "\n",
    "repo_name = 'AudioLDM-training-finetuning'\n",
    "\n",
    "tmp_sub_dir = tmp + repo_name\n",
    "os.makedirs(tmp_sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8d9d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T20:42:27.646698Z",
     "iopub.status.busy": "2024-11-22T20:42:27.646487Z",
     "iopub.status.idle": "2024-11-22T20:42:27.652122Z",
     "shell.execute_reply": "2024-11-22T20:42:27.651464Z"
    },
    "papermill": {
     "duration": 0.01224,
     "end_time": "2024-11-22T20:42:27.653681",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.641441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AudioLDM-training-finetuning']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123e962",
   "metadata": {
    "papermill": {
     "duration": 0.005209,
     "end_time": "2024-11-22T20:42:27.663272",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.658063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Formatting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f424ae",
   "metadata": {
    "papermill": {
     "duration": 0.004205,
     "end_time": "2024-11-22T20:42:27.671812",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.667607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cloning github to temporary directory. Specifically to /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c1de8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T20:42:27.681393Z",
     "iopub.status.busy": "2024-11-22T20:42:27.680854Z",
     "iopub.status.idle": "2024-11-22T20:42:27.684078Z",
     "shell.execute_reply": "2024-11-22T20:42:27.683451Z"
    },
    "papermill": {
     "duration": 0.00963,
     "end_time": "2024-11-22T20:42:27.685604",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.675974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/haoheliu/AudioLDM-training-finetuning.git /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f9db4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T20:42:27.695069Z",
     "iopub.status.busy": "2024-11-22T20:42:27.694864Z",
     "iopub.status.idle": "2024-11-22T20:42:27.697908Z",
     "shell.execute_reply": "2024-11-22T20:42:27.697234Z"
    },
    "papermill": {
     "duration": 0.009516,
     "end_time": "2024-11-22T20:42:27.699426",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.689910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cloned = '/kaggle/tmp/AudioLDM-training-finetuning'\n",
    "\n",
    "# os.listdir(cloned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe3dde9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T20:42:27.708935Z",
     "iopub.status.busy": "2024-11-22T20:42:27.708733Z",
     "iopub.status.idle": "2024-11-22T21:00:33.233242Z",
     "shell.execute_reply": "2024-11-22T21:00:33.232481Z"
    },
    "papermill": {
     "duration": 1085.538675,
     "end_time": "2024-11-22T21:00:33.242441",
     "exception": false,
     "start_time": "2024-11-22T20:42:27.703766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/tmp/AudioLDM-training-finetuning'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_source = '/kaggle/input/project03'\n",
    "dataset_destination = tmp_sub_dir\n",
    "\n",
    "# Copy the dataset directory recursively\n",
    "shutil.copytree(dataset_source, dataset_destination, dirs_exist_ok=True)\n",
    "\n",
    "# os.rename(dataset_destination + '/' + 'project03', dataset_destination + '/' + repo_name)\n",
    "\n",
    "# dataset_source = '/kaggle/input/dcase2024-task-7/dcase_2024_task7'\n",
    "# dataset_destination = '/kaggle/tmp/AudioLDM-training-finetuning/data/dataset'\n",
    "\n",
    "# # Copy the dataset directory recursively\n",
    "# shutil.copytree(dataset_source, dataset_destination, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7a0437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:33.252617Z",
     "iopub.status.busy": "2024-11-22T21:00:33.252329Z",
     "iopub.status.idle": "2024-11-22T21:00:33.255608Z",
     "shell.execute_reply": "2024-11-22T21:00:33.254898Z"
    },
    "papermill": {
     "duration": 0.010189,
     "end_time": "2024-11-22T21:00:33.257181",
     "exception": false,
     "start_time": "2024-11-22T21:00:33.246992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoints_source = '/kaggle/input/checkpoint/checkpoints'\n",
    "# checkpoints_destination = '/kaggle/tmp/AudioLDM-training-finetuning/data/checkpoints'\n",
    "\n",
    "# # Copy the checkpoints directory recursively\n",
    "# shutil.copytree(checkpoints_source, checkpoints_destination, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796cfbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:33.266972Z",
     "iopub.status.busy": "2024-11-22T21:00:33.266753Z",
     "iopub.status.idle": "2024-11-22T21:00:34.296135Z",
     "shell.execute_reply": "2024-11-22T21:00:34.295133Z"
    },
    "papermill": {
     "duration": 1.03638,
     "end_time": "2024-11-22T21:00:34.298030",
     "exception": false,
     "start_time": "2024-11-22T21:00:33.261650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE    audioldm_train  bash_train.sh  log\t       pyproject.toml  tests\r\n",
      "README.md  bash_eval.sh    data\t\t  poetry.lock  taming\r\n"
     ]
    }
   ],
   "source": [
    "# List contents of the dataset directory\n",
    "!ls /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bad51dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:34.308707Z",
     "iopub.status.busy": "2024-11-22T21:00:34.308419Z",
     "iopub.status.idle": "2024-11-22T21:00:35.290508Z",
     "shell.execute_reply": "2024-11-22T21:00:35.289487Z"
    },
    "papermill": {
     "duration": 0.990152,
     "end_time": "2024-11-22T21:00:35.292913",
     "exception": false,
     "start_time": "2024-11-22T21:00:34.302761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audiomae_16k_128bins.ckpt\t\t\t  hifigan_16k_64bins.json\r\n",
      "clap_htsat_tiny.pt\t\t\t\t  hifigan_48k_256bins.ckpt\r\n",
      "clap_music_speech_audioset_epoch_15_esc_89.98.pt  hifigan_48k_256bins.json\r\n",
      "hifigan_16k_64bins.ckpt\t\t\t\t  vae_mel_16k_64bins.ckpt\r\n"
     ]
    }
   ],
   "source": [
    "# List contents of the checkpoints directory\n",
    "!ls /kaggle/tmp/AudioLDM-training-finetuning/data/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008d7890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:35.303824Z",
     "iopub.status.busy": "2024-11-22T21:00:35.303538Z",
     "iopub.status.idle": "2024-11-22T21:00:35.309495Z",
     "shell.execute_reply": "2024-11-22T21:00:35.308812Z"
    },
    "papermill": {
     "duration": 0.013287,
     "end_time": "2024-11-22T21:00:35.311108",
     "exception": false,
     "start_time": "2024-11-22T21:00:35.297821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the root of the cloned repository\n",
    "%cd /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ec5826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:35.321386Z",
     "iopub.status.busy": "2024-11-22T21:00:35.321113Z",
     "iopub.status.idle": "2024-11-22T21:00:37.038090Z",
     "shell.execute_reply": "2024-11-22T21:00:37.037322Z"
    },
    "papermill": {
     "duration": 1.724194,
     "end_time": "2024-11-22T21:00:37.039930",
     "exception": false,
     "start_time": "2024-11-22T21:00:35.315736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files and directories are present\r\n",
      "Checking the validity of the audio datasets\r\n",
      "100%|█████████████████████████████████| 49502/49502 [00:00<00:00, 143075.59it/s]\r\n",
      "100%|█████████████████████████████████████| 964/964 [00:00<00:00, 133091.15it/s]\r\n",
      "All audio files are present. You are good to go!\r\n"
     ]
    }
   ],
   "source": [
    "# Run the validation script\n",
    "!python3 tests/validate_dataset_checkpoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b65b82",
   "metadata": {
    "papermill": {
     "duration": 0.004728,
     "end_time": "2024-11-22T21:00:37.049961",
     "exception": false,
     "start_time": "2024-11-22T21:00:37.045233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setting up the Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ab749d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:37.060957Z",
     "iopub.status.busy": "2024-11-22T21:00:37.060679Z",
     "iopub.status.idle": "2024-11-22T21:00:37.066322Z",
     "shell.execute_reply": "2024-11-22T21:00:37.065632Z"
    },
    "papermill": {
     "duration": 0.013153,
     "end_time": "2024-11-22T21:00:37.067934",
     "exception": false,
     "start_time": "2024-11-22T21:00:37.054781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d62628",
   "metadata": {
    "papermill": {
     "duration": 0.004707,
     "end_time": "2024-11-22T21:00:37.077663",
     "exception": false,
     "start_time": "2024-11-22T21:00:37.072956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "instaling the packages in editable model looks for setup.py or pyproject.toml file in the current directory and installs the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c52daf38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:00:37.088593Z",
     "iopub.status.busy": "2024-11-22T21:00:37.088341Z",
     "iopub.status.idle": "2024-11-22T21:01:42.996841Z",
     "shell.execute_reply": "2024-11-22T21:01:42.995899Z"
    },
    "papermill": {
     "duration": 65.916595,
     "end_time": "2024-11-22T21:01:42.999133",
     "exception": false,
     "start_time": "2024-11-22T21:00:37.082538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/tmp/AudioLDM-training-finetuning\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git (from audioldm-train==0.1.0)\r\n",
      "  Cloning https://github.com/haoheliu/audioldm_eval.git to /tmp/pip-install-uiuntigl/audioldm-eval_2e2a7d540542466386753cbbc9b51776\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/haoheliu/audioldm_eval.git /tmp/pip-install-uiuntigl/audioldm-eval_2e2a7d540542466386753cbbc9b51776\r\n",
      "  Resolved https://github.com/haoheliu/audioldm_eval.git to commit 8dc07ee7c42f9dc6e295460a1034175a0d49b436\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting hear21passt@ git+https://github.com/haoheliu/passt_hear21.git (from audioldm-train==0.1.0)\r\n",
      "  Cloning https://github.com/haoheliu/passt_hear21.git to /tmp/pip-install-uiuntigl/hear21passt_55003c084cc9473fa5f0eaf241193fdd\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/haoheliu/passt_hear21.git /tmp/pip-install-uiuntigl/hear21passt_55003c084cc9473fa5f0eaf241193fdd\r\n",
      "  Resolved https://github.com/haoheliu/passt_hear21.git to commit 4dd6b9e426f528e2e8409b9bacecf58a2f464548\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hCollecting braceexpand<0.2.0,>=0.1.7 (from audioldm-train==0.1.0)\r\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting einops<0.8.0,>=0.7.0 (from audioldm-train==0.1.0)\r\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting ftfy<7.0.0,>=6.1.1 (from audioldm-train==0.1.0)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from audioldm-train==0.1.0) (3.11.0)\r\n",
      "Collecting ipdb<0.14.0,>=0.13.13 (from audioldm-train==0.1.0)\r\n",
      "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: kornia<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from audioldm-train==0.1.0) (0.7.3)\r\n",
      "Collecting matplotlib<4.0.0,>=3.8.1 (from audioldm-train==0.1.0)\r\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.3 in /opt/conda/lib/python3.10/site-packages (from audioldm-train==0.1.0) (2.2.2)\r\n",
      "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from audioldm-train==0.1.0) (2.4.0)\r\n",
      "Requirement already satisfied: ruamel-yaml<0.19.0,>=0.18.5 in /opt/conda/lib/python3.10/site-packages (from audioldm-train==0.1.0) (0.18.6)\r\n",
      "Collecting taming-transformers-rom1504<0.0.7,>=0.0.6 (from audioldm-train==0.1.0)\r\n",
      "  Downloading taming_transformers_rom1504-0.0.6-py3-none-any.whl.metadata (406 bytes)\r\n",
      "Collecting transformers==4.30.2 (from audioldm-train==0.1.0)\r\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting wandb<0.17.0,>=0.16.0 (from audioldm-train==0.1.0)\r\n",
      "  Downloading wandb-0.16.6-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting webdataset<0.3.0,>=0.2.75 (from audioldm-train==0.1.0)\r\n",
      "  Downloading webdataset-0.2.100-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting wget<4.0,>=3.2 (from audioldm-train==0.1.0)\r\n",
      "  Downloading wget-3.2.zip (10 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (3.15.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (0.25.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (2024.5.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (2.32.3)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->audioldm-train==0.1.0)\r\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->audioldm-train==0.1.0) (4.66.4)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy<7.0.0,>=6.1.1->audioldm-train==0.1.0) (0.2.13)\r\n",
      "Requirement already satisfied: ipython>=7.31.1 in /opt/conda/lib/python3.10/site-packages (from ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (8.21.0)\r\n",
      "Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (2.0.1)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (5.1.1)\r\n",
      "Requirement already satisfied: kornia-rs>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (0.1.5)\r\n",
      "Requirement already satisfied: torch>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (2.4.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4.0.0,>=3.8.1->audioldm-train==0.1.0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.3->audioldm-train==0.1.0) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.3->audioldm-train==0.1.0) (2024.1)\r\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (2024.6.1)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (1.4.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (4.12.2)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (0.11.7)\r\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from ruamel-yaml<0.19.0,>=0.18.5->audioldm-train==0.1.0) (0.2.8)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from taming-transformers-rom1504<0.0.7,>=0.0.6->audioldm-train==0.1.0) (0.19.0)\r\n",
      "Collecting omegaconf>=2.0.0 (from taming-transformers-rom1504<0.0.7,>=0.0.6->audioldm-train==0.1.0)\r\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (3.1.43)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (5.9.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (2.15.0)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (0.4.0)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (70.0.0)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (1.4.4)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (3.20.3)\r\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.23.2)\r\n",
      "Collecting torchlibrosa (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0)\r\n",
      "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.4.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.14.1)\r\n",
      "Collecting ssr_eval (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0)\r\n",
      "  Downloading ssr_eval-0.0.7-py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.10.2.post1)\r\n",
      "Collecting timm==0.4.12 (from hear21passt@ git+https://github.com/haoheliu/passt_hear21.git->audioldm-train==0.1.0)\r\n",
      "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (1.16.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (3.9.5)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (4.0.11)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.6.2)\r\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (5.14.3)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (4.9.0)\r\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0.0->taming-transformers-rom1504<0.0.7,>=0.0.6->audioldm-train==0.1.0)\r\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->audioldm-train==0.1.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->audioldm-train==0.1.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->audioldm-train==0.1.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->audioldm-train==0.1.0) (2024.8.30)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.1->kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (3.1.4)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (3.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.4.2)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.60.0)\r\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.12.1)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.8.2)\r\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.5.0.post1)\r\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.0.8)\r\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (2.34.1)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (2024.5.22)\r\n",
      "Collecting wave (from ssr_eval->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0)\r\n",
      "  Downloading Wave-0.0.2.zip (38 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.1.1->audioldm-train==0.1.0) (4.0.3)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb<0.17.0,>=0.16.0->audioldm-train==0.1.0) (5.0.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.8.4)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (0.43.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.7.0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (3.11.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (3.5.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.1->kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (2.1.5)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb<0.14.0,>=0.13.13->audioldm-train==0.1.0) (0.2.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.1->kornia<0.8.0,>=0.7.0->audioldm-train==0.1.0) (1.3.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->audioldm-eval@ git+https://github.com/haoheliu/audioldm_eval.git->audioldm-train==0.1.0) (2.22)\r\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\r\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ipdb-0.13.13-py3-none-any.whl (12 kB)\r\n",
      "Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading taming_transformers_rom1504-0.0.6-py3-none-any.whl (51 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading webdataset-0.2.100-py3-none-any.whl (74 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ssr_eval-0.0.7-py3-none-any.whl (15 kB)\r\n",
      "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\r\n",
      "Building wheels for collected packages: audioldm-train, wget, audioldm-eval, hear21passt, antlr4-python3-runtime, wave\r\n",
      "  Building editable for audioldm-train (pyproject.toml) ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Created wheel for audioldm-train: filename=audioldm_train-0.1.0-py3-none-any.whl size=5085 sha256=c6d153d06f935f44842fef569ca480c6a1049dbcc8f734f4fee2be44817fd1ee\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-agtjks09/wheels/3e/2c/15/2014bb1110cdbfa4e3f279db2cb1a5e7b758d3cff930807e84\r\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=180a71c8f6c1a5c3dc5ca61f3bfc4bdd4c8d6934c35d8126a50f6eddbe0f518d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\r\n",
      "  Building wheel for audioldm-eval (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for audioldm-eval: filename=audioldm_eval-0.0.5-py3-none-any.whl size=67444 sha256=ee139d0d9f1540f07f066859025a3823309c567c1c2afdc9cc33e440cf5e71fd\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-agtjks09/wheels/c4/2f/dd/8c3ba5638f1628e4b2ec463099ff5fd139a0beac69a0b3a824\r\n",
      "  Building wheel for hear21passt (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for hear21passt: filename=hear21passt-0.0.23-py3-none-any.whl size=31851 sha256=c72d2907c9ae7a2fdc2f120a4ec576d07dbf9b49f1c95c7ab0093d545f3f3cf0\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-agtjks09/wheels/a0/d7/a0/183bd01c88f0a57ced05035b3756492e163b86f2631c615ada\r\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=25e93a9566d32374ac4b0fa72bbb0ac95ad186aa390074961f1e815c68ad025a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\r\n",
      "  Building wheel for wave (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for wave: filename=Wave-0.0.2-py3-none-any.whl size=1220 sha256=87a605825d78c36e976491fa4df4677cfad9f8304f8a9f0ef605fc4d8ea2d787\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f8/24/4d/1b01c0e32da3eb3fd71bbbc6093fcc557ec3b2d9e532ecd65d\r\n",
      "Successfully built audioldm-train wget audioldm-eval hear21passt antlr4-python3-runtime wave\r\n",
      "Installing collected packages: wget, wave, tokenizers, braceexpand, antlr4-python3-runtime, webdataset, omegaconf, ftfy, einops, matplotlib, wandb, transformers, torchlibrosa, timm, ipdb, taming-transformers-rom1504, ssr_eval, hear21passt, audioldm-eval, audioldm-train\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: matplotlib\r\n",
      "    Found existing installation: matplotlib 3.7.5\r\n",
      "    Uninstalling matplotlib-3.7.5:\r\n",
      "      Successfully uninstalled matplotlib-3.7.5\r\n",
      "  Attempting uninstall: wandb\r\n",
      "    Found existing installation: wandb 0.18.3\r\n",
      "    Uninstalling wandb-0.18.3:\r\n",
      "      Successfully uninstalled wandb-0.18.3\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.9\r\n",
      "    Uninstalling timm-1.0.9:\r\n",
      "      Successfully uninstalled timm-1.0.9\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "beatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\r\n",
      "kaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\r\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 audioldm-eval-0.0.5 audioldm-train-0.1.0 braceexpand-0.1.7 einops-0.7.0 ftfy-6.3.1 hear21passt-0.0.23 ipdb-0.13.13 matplotlib-3.9.2 omegaconf-2.3.0 ssr_eval-0.0.7 taming-transformers-rom1504-0.0.6 timm-0.4.12 tokenizers-0.13.3 torchlibrosa-0.1.0 transformers-4.30.2 wandb-0.16.6 wave-0.0.2 webdataset-0.2.100 wget-3.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bd9e67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:43.021012Z",
     "iopub.status.busy": "2024-11-22T21:01:43.020718Z",
     "iopub.status.idle": "2024-11-22T21:01:43.027176Z",
     "shell.execute_reply": "2024-11-22T21:01:43.026265Z"
    },
    "papermill": {
     "duration": 0.019182,
     "end_time": "2024-11-22T21:01:43.028792",
     "exception": false,
     "start_time": "2024-11-22T21:01:43.009610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 'audioldm_train' imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Try importing the module to check if it's installed correctly\n",
    "try:\n",
    "    import audioldm_train\n",
    "    print(\"Module 'audioldm_train' imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea5bef",
   "metadata": {
    "papermill": {
     "duration": 0.009794,
     "end_time": "2024-11-22T21:01:43.048684",
     "exception": false,
     "start_time": "2024-11-22T21:01:43.038890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **RUN THIS B4 TRRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a2648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:43.069818Z",
     "iopub.status.busy": "2024-11-22T21:01:43.069198Z",
     "iopub.status.idle": "2024-11-22T21:01:51.801249Z",
     "shell.execute_reply": "2024-11-22T21:01:51.800059Z"
    },
    "papermill": {
     "duration": 8.744858,
     "end_time": "2024-11-22T21:01:51.803437",
     "exception": false,
     "start_time": "2024-11-22T21:01:43.058579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa==0.9.2\r\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (3.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (1.14.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (1.2.2)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (1.4.2)\r\n",
      "Requirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (5.1.1)\r\n",
      "Collecting resampy>=0.2.2 (from librosa==0.9.2)\r\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: numba>=0.45.1 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (0.60.0)\r\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (0.12.1)\r\n",
      "Requirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (1.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from librosa==0.9.2) (21.3)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.2) (0.43.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->librosa==0.9.2) (3.1.2)\r\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2) (3.11.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.2) (2.32.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.2) (3.5.0)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.10.2->librosa==0.9.2) (1.16.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2) (2.22)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2024.8.30)\r\n",
      "Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: resampy, librosa\r\n",
      "  Attempting uninstall: librosa\r\n",
      "    Found existing installation: librosa 0.10.2.post1\r\n",
      "    Uninstalling librosa-0.10.2.post1:\r\n",
      "      Successfully uninstalled librosa-0.10.2.post1\r\n",
      "Successfully installed librosa-0.9.2 resampy-0.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa==0.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9e199",
   "metadata": {
    "papermill": {
     "duration": 0.010961,
     "end_time": "2024-11-22T21:01:51.826008",
     "exception": false,
     "start_time": "2024-11-22T21:01:51.815047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Running Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0aa6e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:51.849367Z",
     "iopub.status.busy": "2024-11-22T21:01:51.849034Z",
     "iopub.status.idle": "2024-11-22T21:01:51.854985Z",
     "shell.execute_reply": "2024-11-22T21:01:51.854260Z"
    },
    "papermill": {
     "duration": 0.019406,
     "end_time": "2024-11-22T21:01:51.856440",
     "exception": false,
     "start_time": "2024-11-22T21:01:51.837034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're in the root directory\n",
    "%cd /kaggle/tmp/AudioLDM-training-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017d2fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:51.878268Z",
     "iopub.status.busy": "2024-11-22T21:01:51.877815Z",
     "iopub.status.idle": "2024-11-22T21:01:51.884797Z",
     "shell.execute_reply": "2024-11-22T21:01:51.884037Z"
    },
    "papermill": {
     "duration": 0.019611,
     "end_time": "2024-11-22T21:01:51.886290",
     "exception": false,
     "start_time": "2024-11-22T21:01:51.866679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_to_write = '''\n",
    "metadata_root: \"./data/dataset/metadata/dataset_root.json\"\n",
    "log_directory: \"./log/latent_diffusion\"\n",
    "project: \"audioldm\"\n",
    "precision: \"high\"\n",
    "\n",
    "variables:\n",
    "  sampling_rate: &sampling_rate 16000 \n",
    "  mel_bins: &mel_bins 64\n",
    "  latent_embed_dim: &latent_embed_dim 8\n",
    "  latent_t_size: &latent_t_size 256 # TODO might need to change\n",
    "  latent_f_size: &latent_f_size 16\n",
    "  in_channels: &unet_in_channels 8\n",
    "  optimize_ddpm_parameter: &optimize_ddpm_parameter true\n",
    "  optimize_gpt: &optimize_gpt true\n",
    "  warmup_steps: &warmup_steps 2000\n",
    "\n",
    "data: \n",
    "  train: [\"audiocaps\"]\n",
    "  val: \"audiocaps\"\n",
    "  test: \"audiocaps\"\n",
    "  class_label_indices: \"audioset_eval_subset\"\n",
    "  dataloader_add_ons: [] \n",
    "\n",
    "step:\n",
    "  validation_every_n_epochs: 10\n",
    "  save_checkpoint_every_n_steps: 45000\n",
    "  # limit_val_batches: 2\n",
    "  max_steps: 45000\n",
    "  save_top_k: 1\n",
    "\n",
    "preprocessing:\n",
    "  audio:\n",
    "    sampling_rate: *sampling_rate\n",
    "    max_wav_value: 32768.0\n",
    "    duration: 10.24\n",
    "  stft:\n",
    "    filter_length: 1024\n",
    "    hop_length: 160\n",
    "    win_length: 1024\n",
    "  mel:\n",
    "    n_mel_channels: *mel_bins\n",
    "    mel_fmin: 0\n",
    "    mel_fmax: 8000 \n",
    "\n",
    "augmentation:\n",
    "  mixup: 0.0\n",
    "\n",
    "model:\n",
    "  target: audioldm_train.modules.latent_diffusion.ddpm.LatentDiffusion\n",
    "  params: \n",
    "    # Autoencoder\n",
    "    first_stage_config:\n",
    "      base_learning_rate: 8.0e-06\n",
    "      target: audioldm_train.modules.latent_encoder.autoencoder.AutoencoderKL\n",
    "      params: \n",
    "        reload_from_ckpt: \"data/checkpoints/vae_mel_16k_64bins.ckpt\"\n",
    "        sampling_rate: *sampling_rate\n",
    "        batchsize: 4\n",
    "        monitor: val/rec_loss\n",
    "        image_key: fbank\n",
    "        subband: 1\n",
    "        embed_dim: *latent_embed_dim\n",
    "        time_shuffle: 1\n",
    "        lossconfig:\n",
    "          target: audioldm_train.losses.LPIPSWithDiscriminator\n",
    "          params:\n",
    "            disc_start: 50001\n",
    "            kl_weight: 1000.0\n",
    "            disc_weight: 0.5\n",
    "            disc_in_channels: 1\n",
    "        ddconfig: \n",
    "          double_z: true\n",
    "          mel_bins: *mel_bins # The frequency bins of mel spectrogram\n",
    "          z_channels: 8\n",
    "          resolution: 256\n",
    "          downsample_time: false\n",
    "          in_channels: 1\n",
    "          out_ch: 1\n",
    "          ch: 128 \n",
    "          ch_mult:\n",
    "          - 1\n",
    "          - 2\n",
    "          - 4\n",
    "          num_res_blocks: 2\n",
    "          attn_resolutions: []\n",
    "          dropout: 0.0\n",
    "    \n",
    "    # Other parameters\n",
    "    base_learning_rate: 1.0e-4\n",
    "    warmup_steps: *warmup_steps\n",
    "    optimize_ddpm_parameter: *optimize_ddpm_parameter\n",
    "    sampling_rate: *sampling_rate\n",
    "    batchsize: 2\n",
    "    linear_start: 0.0015\n",
    "    linear_end: 0.0195\n",
    "    num_timesteps_cond: 1\n",
    "    log_every_t: 200\n",
    "    timesteps: 1000\n",
    "    unconditional_prob_cfg: 0.1\n",
    "    parameterization: eps # [eps, x0, v]\n",
    "    first_stage_key: fbank\n",
    "    latent_t_size: *latent_t_size # TODO might need to change\n",
    "    latent_f_size: *latent_f_size\n",
    "    channels: *latent_embed_dim # TODO might need to change\n",
    "    monitor: val/loss_simple_ema\n",
    "    scale_by_std: true\n",
    "    unet_config:\n",
    "      target: audioldm_train.modules.diffusionmodules.openaimodel.UNetModel\n",
    "      params:\n",
    "        image_size: 64 \n",
    "        extra_film_condition_dim: 512 # If you use film as extra condition, set this parameter. For example if you have two conditioning vectors each have dimension 512, then this number would be 1024\n",
    "        # context_dim: \n",
    "        # - 768\n",
    "        in_channels: *unet_in_channels # The input channel of the UNet model\n",
    "        out_channels: *latent_embed_dim # TODO might need to change\n",
    "        model_channels: 128 # TODO might need to change\n",
    "        attention_resolutions:\n",
    "        - 8\n",
    "        - 4\n",
    "        - 2\n",
    "        num_res_blocks: 2\n",
    "        channel_mult: \n",
    "        - 1\n",
    "        - 2\n",
    "        - 3\n",
    "        - 5\n",
    "        num_head_channels: 32\n",
    "        use_spatial_transformer: true\n",
    "        transformer_depth: 1\n",
    "        extra_sa_layer: false\n",
    "    \n",
    "    cond_stage_config:\n",
    "      film_clap_cond1:\n",
    "        cond_stage_key: text\n",
    "        conditioning_key: film\n",
    "        target: audioldm_train.conditional_models.CLAPAudioEmbeddingClassifierFreev2\n",
    "        params:\n",
    "          pretrained_path: data/checkpoints/clap_htsat_tiny.pt\n",
    "          sampling_rate: 16000\n",
    "          embed_mode: text # or text\n",
    "          amodel: HTSAT-tiny\n",
    "\n",
    "    evaluation_params:\n",
    "      unconditional_guidance_scale: 3.5\n",
    "      ddim_sampling_steps: 200\n",
    "      n_candidates_per_samples: 3\n",
    "'''\n",
    "\n",
    "yaml_path = '/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original.yaml'\n",
    "\n",
    "# os.listdir('/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/config/2023_08_23_reproduce_audioldm')\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml_file.write(str_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94e06294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:51.908656Z",
     "iopub.status.busy": "2024-11-22T21:01:51.907879Z",
     "iopub.status.idle": "2024-11-22T21:01:52.894338Z",
     "shell.execute_reply": "2024-11-22T21:01:52.893531Z"
    },
    "papermill": {
     "duration": 0.999326,
     "end_time": "2024-11-22T21:01:52.896120",
     "exception": false,
     "start_time": "2024-11-22T21:01:51.896794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "metadata_root: \"./data/dataset/metadata/dataset_root.json\"\r\n",
      "log_directory: \"./log/latent_diffusion\"\r\n",
      "project: \"audioldm\"\r\n",
      "precision: \"high\"\r\n",
      "\r\n",
      "variables:\r\n",
      "  sampling_rate: &sampling_rate 16000 \r\n",
      "  mel_bins: &mel_bins 64\r\n",
      "  latent_embed_dim: &latent_embed_dim 8\r\n",
      "  latent_t_size: &latent_t_size 256 # TODO might need to change\r\n",
      "  latent_f_size: &latent_f_size 16\r\n",
      "  in_channels: &unet_in_channels 8\r\n",
      "  optimize_ddpm_parameter: &optimize_ddpm_parameter true\r\n",
      "  optimize_gpt: &optimize_gpt true\r\n",
      "  warmup_steps: &warmup_steps 2000\r\n",
      "\r\n",
      "data: \r\n",
      "  train: [\"audiocaps\"]\r\n",
      "  val: \"audiocaps\"\r\n",
      "  test: \"audiocaps\"\r\n",
      "  class_label_indices: \"audioset_eval_subset\"\r\n",
      "  dataloader_add_ons: [] \r\n",
      "\r\n",
      "step:\r\n",
      "  validation_every_n_epochs: 10\r\n",
      "  save_checkpoint_every_n_steps: 45000\r\n",
      "  # limit_val_batches: 2\r\n",
      "  max_steps: 45000\r\n",
      "  save_top_k: 1\r\n",
      "\r\n",
      "preprocessing:\r\n",
      "  audio:\r\n",
      "    sampling_rate: *sampling_rate\r\n",
      "    max_wav_value: 32768.0\r\n",
      "    duration: 10.24\r\n",
      "  stft:\r\n",
      "    filter_length: 1024\r\n",
      "    hop_length: 160\r\n",
      "    win_length: 1024\r\n",
      "  mel:\r\n",
      "    n_mel_channels: *mel_bins\r\n",
      "    mel_fmin: 0\r\n",
      "    mel_fmax: 8000 \r\n",
      "\r\n",
      "augmentation:\r\n",
      "  mixup: 0.0\r\n",
      "\r\n",
      "model:\r\n",
      "  target: audioldm_train.modules.latent_diffusion.ddpm.LatentDiffusion\r\n",
      "  params: \r\n",
      "    # Autoencoder\r\n",
      "    first_stage_config:\r\n",
      "      base_learning_rate: 8.0e-06\r\n",
      "      target: audioldm_train.modules.latent_encoder.autoencoder.AutoencoderKL\r\n",
      "      params: \r\n",
      "        reload_from_ckpt: \"data/checkpoints/vae_mel_16k_64bins.ckpt\"\r\n",
      "        sampling_rate: *sampling_rate\r\n",
      "        batchsize: 4\r\n",
      "        monitor: val/rec_loss\r\n",
      "        image_key: fbank\r\n",
      "        subband: 1\r\n",
      "        embed_dim: *latent_embed_dim\r\n",
      "        time_shuffle: 1\r\n",
      "        lossconfig:\r\n",
      "          target: audioldm_train.losses.LPIPSWithDiscriminator\r\n",
      "          params:\r\n",
      "            disc_start: 50001\r\n",
      "            kl_weight: 1000.0\r\n",
      "            disc_weight: 0.5\r\n",
      "            disc_in_channels: 1\r\n",
      "        ddconfig: \r\n",
      "          double_z: true\r\n",
      "          mel_bins: *mel_bins # The frequency bins of mel spectrogram\r\n",
      "          z_channels: 8\r\n",
      "          resolution: 256\r\n",
      "          downsample_time: false\r\n",
      "          in_channels: 1\r\n",
      "          out_ch: 1\r\n",
      "          ch: 128 \r\n",
      "          ch_mult:\r\n",
      "          - 1\r\n",
      "          - 2\r\n",
      "          - 4\r\n",
      "          num_res_blocks: 2\r\n",
      "          attn_resolutions: []\r\n",
      "          dropout: 0.0\r\n",
      "    \r\n",
      "    # Other parameters\r\n",
      "    base_learning_rate: 1.0e-4\r\n",
      "    warmup_steps: *warmup_steps\r\n",
      "    optimize_ddpm_parameter: *optimize_ddpm_parameter\r\n",
      "    sampling_rate: *sampling_rate\r\n",
      "    batchsize: 2\r\n",
      "    linear_start: 0.0015\r\n",
      "    linear_end: 0.0195\r\n",
      "    num_timesteps_cond: 1\r\n",
      "    log_every_t: 200\r\n",
      "    timesteps: 1000\r\n",
      "    unconditional_prob_cfg: 0.1\r\n",
      "    parameterization: eps # [eps, x0, v]\r\n",
      "    first_stage_key: fbank\r\n",
      "    latent_t_size: *latent_t_size # TODO might need to change\r\n",
      "    latent_f_size: *latent_f_size\r\n",
      "    channels: *latent_embed_dim # TODO might need to change\r\n",
      "    monitor: val/loss_simple_ema\r\n",
      "    scale_by_std: true\r\n",
      "    unet_config:\r\n",
      "      target: audioldm_train.modules.diffusionmodules.openaimodel.UNetModel\r\n",
      "      params:\r\n",
      "        image_size: 64 \r\n",
      "        extra_film_condition_dim: 512 # If you use film as extra condition, set this parameter. For example if you have two conditioning vectors each have dimension 512, then this number would be 1024\r\n",
      "        # context_dim: \r\n",
      "        # - 768\r\n",
      "        in_channels: *unet_in_channels # The input channel of the UNet model\r\n",
      "        out_channels: *latent_embed_dim # TODO might need to change\r\n",
      "        model_channels: 128 # TODO might need to change\r\n",
      "        attention_resolutions:\r\n",
      "        - 8\r\n",
      "        - 4\r\n",
      "        - 2\r\n",
      "        num_res_blocks: 2\r\n",
      "        channel_mult: \r\n",
      "        - 1\r\n",
      "        - 2\r\n",
      "        - 3\r\n",
      "        - 5\r\n",
      "        num_head_channels: 32\r\n",
      "        use_spatial_transformer: true\r\n",
      "        transformer_depth: 1\r\n",
      "        extra_sa_layer: false\r\n",
      "    \r\n",
      "    cond_stage_config:\r\n",
      "      film_clap_cond1:\r\n",
      "        cond_stage_key: text\r\n",
      "        conditioning_key: film\r\n",
      "        target: audioldm_train.conditional_models.CLAPAudioEmbeddingClassifierFreev2\r\n",
      "        params:\r\n",
      "          pretrained_path: data/checkpoints/clap_htsat_tiny.pt\r\n",
      "          sampling_rate: 16000\r\n",
      "          embed_mode: text # or text\r\n",
      "          amodel: HTSAT-tiny\r\n",
      "\r\n",
      "    evaluation_params:\r\n",
      "      unconditional_guidance_scale: 3.5\r\n",
      "      ddim_sampling_steps: 200\r\n",
      "      n_candidates_per_samples: 3\r\n"
     ]
    }
   ],
   "source": [
    "!cat /kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5950cc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T21:01:52.919130Z",
     "iopub.status.busy": "2024-11-22T21:01:52.918831Z",
     "iopub.status.idle": "2024-11-23T08:02:24.550156Z",
     "shell.execute_reply": "2024-11-23T08:02:24.549210Z"
    },
    "papermill": {
     "duration": 39631.644972,
     "end_time": "2024-11-23T08:02:24.552192",
     "exception": false,
     "start_time": "2024-11-22T21:01:52.907220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED EVERYTHING TO 0\r\n",
      "Add-ons: []\r\n",
      "Build dataset split train from ['audiocaps']\r\n",
      "Data size: 49502\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  fft_window = pad_center(fft_window, filter_length)\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:145: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  mel_basis = librosa_mel_fn(\r\n",
      "Dataset initialize finished\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(_create_warning_msg(\r\n",
      "The length of the dataset is 49502, the length of the dataloader is 24751, the batchsize is 2\r\n",
      "Add-ons: []\r\n",
      "Build dataset split test from audiocaps\r\n",
      "Data size: 964\r\n",
      "Dataset initialize finished\r\n",
      "Train from scratch\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 4.62MB/s]\r\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 3.50MB/s]\r\n",
      "tokenizer_config.json: 100%|██████████████████| 25.0/25.0 [00:00<00:00, 189kB/s]\r\n",
      "config.json: 100%|█████████████████████████████| 481/481 [00:00<00:00, 3.85MB/s]\r\n",
      "LatentDiffusion: Running in eps-prediction mode\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\r\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n",
      "model.safetensors: 100%|██████████████████████| 499M/499M [00:02<00:00, 235MB/s]\r\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\r\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/clap/open_clip/factory.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\r\n",
      "  warnings.warn(\r\n",
      "+ Use extra condition on UNet channel using Film. Extra condition dimension is 512. \r\n",
      "DiffusionWrapper has 185.04 M params.\r\n",
      "Keeping EMAs of 692.\r\n",
      "making attention of type 'vanilla' with 512 in_channels\r\n",
      "making attention of type 'vanilla' with 512 in_channels\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\r\n",
      "  warnings.warn(msg)\r\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\r\n",
      "100%|█████████████████████████████████████████| 528M/528M [00:02<00:00, 226MB/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/taming/modules/losses/lpips.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\r\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\r\n",
      "  WeightNorm.apply(module, name, dim)\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/model_util.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  ckpt = torch.load(model_path + \".ckpt\")\r\n",
      "Removing weight norm...\r\n",
      "Initial learning rate 1e-05\r\n",
      "--> Reload weight of autoencoder from data/checkpoints/vae_mel_16k_64bins.ckpt\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/latent_encoder/autoencoder.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(self.reload_from_ckpt)\r\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\r\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "==> Save checkpoint every 45000 steps\r\n",
      "==> Perform validation every 10 epochs\r\n",
      "SEED EVERYTHING TO 0\r\n",
      "Add-ons: []\r\n",
      "Build dataset split train from ['audiocaps']\r\n",
      "Data size: 49502\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  fft_window = pad_center(fft_window, filter_length)\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:145: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  mel_basis = librosa_mel_fn(\r\n",
      "Dataset initialize finished\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(_create_warning_msg(\r\n",
      "The length of the dataset is 49502, the length of the dataloader is 24751, the batchsize is 2\r\n",
      "Add-ons: []\r\n",
      "Build dataset split test from audiocaps\r\n",
      "Data size: 964\r\n",
      "Dataset initialize finished\r\n",
      "Train from scratch\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "LatentDiffusion: Running in eps-prediction mode\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\r\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\r\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\r\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/clap/open_clip/factory.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\r\n",
      "  warnings.warn(\r\n",
      "+ Use extra condition on UNet channel using Film. Extra condition dimension is 512. \r\n",
      "DiffusionWrapper has 185.04 M params.\r\n",
      "Keeping EMAs of 692.\r\n",
      "making attention of type 'vanilla' with 512 in_channels\r\n",
      "making attention of type 'vanilla' with 512 in_channels\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\r\n",
      "  warnings.warn(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\r\n",
      "  warnings.warn(msg)\r\n",
      "/opt/conda/lib/python3.10/site-packages/taming/modules/losses/lpips.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\r\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\r\n",
      "  WeightNorm.apply(module, name, dim)\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/model_util.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  ckpt = torch.load(model_path + \".ckpt\")\r\n",
      "Removing weight norm...\r\n",
      "Initial learning rate 1e-05\r\n",
      "--> Reload weight of autoencoder from data/checkpoints/vae_mel_16k_64bins.ckpt\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/latent_encoder/autoencoder.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\r\n",
      "  checkpoint = torch.load(self.reload_from_ckpt)\r\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\r\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "==> Save checkpoint every 45000 steps\r\n",
      "==> Perform validation every 10 epochs\r\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]Change the model original cond_keyand embed_mode text, text to text during evaluation\r\n",
      "Change the model original cond_keyand embed_mode text, text to text during evaluation\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/data/dataset.py:455: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  mel = librosa_mel_fn(\r\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/data/dataset.py:455: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\r\n",
      "  mel = librosa_mel_fn(\r\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/1 [00:00<?, ?it/s]Waveform inference save path:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/infer_11-22-21:02_cfg_scale_3.5_ddim_200_n_cand_3\r\n",
      "Waveform inference save path:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/infer_11-22-21:02_cfg_scale_3.5_ddim_200_n_cand_3\r\n",
      "Plotting: Switched to EMA weights\r\n",
      "Plotting: Switched to EMA weights\r\n",
      "Warning: CLAP model normally should use text for evaluation\r\n",
      "Warning: CLAP model normally should use text for evaluation\r\n",
      "Use ddim sampler\r\n",
      "Use ddim sampler\r\n",
      "Data shape for DDIM sampling is (24, 8, 256, 16), eta 1.0\r\n",
      "Data shape for DDIM sampling is (24, 8, 256, 16), eta 1.0\r\n",
      "Running DDIM Sampling with 200 timesteps\r\n",
      "Running DDIM Sampling with 200 timesteps\r\n",
      "\r\n",
      "DDIM Sampler:   0%|                                     | 0/200 [00:00<?, ?it/s]\u001b[AThe shape of UNet input is torch.Size([24, 8, 256, 16])\r\n",
      "DDIM Sampler:   0%|                                     | 0/200 [00:00<?, ?it/s]The shape of UNet input is torch.Size([24, 8, 256, 16])\r\n",
      "DDIM Sampler:   0%|▏                            | 1/200 [00:01<05:35,  1.68s/it]\r\n",
      "DDIM Sampler:   1%|▎                            | 2/200 [00:03<05:23,  1.63s/it]\r\n",
      "DDIM Sampler:   2%|▍                            | 3/200 [00:04<05:18,  1.62s/it]\r\n",
      "DDIM Sampler:   2%|▌                            | 4/200 [00:06<05:15,  1.61s/it]\r\n",
      "DDIM Sampler:   2%|▋                            | 5/200 [00:08<05:13,  1.61s/it]\r\n",
      "DDIM Sampler:   3%|▊                            | 6/200 [00:09<05:11,  1.61s/it]\r\n",
      "DDIM Sampler:   4%|█                            | 7/200 [00:11<05:10,  1.61s/it]\r\n",
      "DDIM Sampler:   4%|█▏                           | 8/200 [00:12<05:09,  1.61s/it]\r\n",
      "DDIM Sampler:   4%|█▎                           | 9/200 [00:14<05:07,  1.61s/it]\r\n",
      "DDIM Sampler:   5%|█▍                          | 10/200 [00:16<05:06,  1.61s/it]\r\n",
      "DDIM Sampler:   6%|█▌                          | 11/200 [00:17<05:05,  1.61s/it]\r\n",
      "DDIM Sampler:   6%|█▋                          | 12/200 [00:19<05:04,  1.62s/it]\r\n",
      "DDIM Sampler:   6%|█▊                          | 13/200 [00:21<05:02,  1.62s/it]\r\n",
      "DDIM Sampler:   7%|█▉                          | 14/200 [00:22<05:01,  1.62s/it]\r\n",
      "DDIM Sampler:   8%|██                          | 15/200 [00:24<05:01,  1.63s/it]\r\n",
      "DDIM Sampler:   8%|██▏                         | 16/200 [00:25<05:00,  1.63s/it]\r\n",
      "DDIM Sampler:   8%|██▍                         | 17/200 [00:27<04:58,  1.63s/it]\r\n",
      "DDIM Sampler:   9%|██▌                         | 18/200 [00:29<04:57,  1.64s/it]\r\n",
      "DDIM Sampler:  10%|██▋                         | 19/200 [00:30<04:56,  1.64s/it]\r\n",
      "DDIM Sampler:  10%|██▉                         | 21/200 [00:34<04:54,  1.65s/it]\r\n",
      "DDIM Sampler:  11%|███                         | 22/200 [00:35<04:53,  1.65s/it]\r\n",
      "DDIM Sampler:  12%|███▏                        | 23/200 [00:37<04:52,  1.65s/it]\r\n",
      "DDIM Sampler:  12%|███▎                        | 24/200 [00:39<04:51,  1.66s/it]\r\n",
      "DDIM Sampler:  12%|███▌                        | 25/200 [00:40<04:50,  1.66s/it]\r\n",
      "DDIM Sampler:  13%|███▋                        | 26/200 [00:42<04:49,  1.66s/it]\r\n",
      "DDIM Sampler:  14%|███▊                        | 27/200 [00:44<04:48,  1.67s/it]\r\n",
      "DDIM Sampler:  14%|███▉                        | 28/200 [00:45<04:47,  1.67s/it]\r\n",
      "DDIM Sampler:  14%|████                        | 29/200 [00:47<04:46,  1.68s/it]\r\n",
      "DDIM Sampler:  15%|████▏                       | 30/200 [00:49<04:46,  1.68s/it]\r\n",
      "DDIM Sampler:  16%|████▎                       | 31/200 [00:50<04:45,  1.69s/it]\r\n",
      "DDIM Sampler:  16%|████▍                       | 32/200 [00:52<04:44,  1.69s/it]\r\n",
      "DDIM Sampler:  17%|████▊                       | 34/200 [00:56<04:41,  1.70s/it]\r\n",
      "DDIM Sampler:  18%|████▉                       | 35/200 [00:57<04:41,  1.71s/it]\r\n",
      "DDIM Sampler:  18%|█████                       | 36/200 [00:59<04:41,  1.72s/it]\r\n",
      "DDIM Sampler:  18%|█████▏                      | 37/200 [01:01<04:41,  1.72s/it]\r\n",
      "DDIM Sampler:  19%|█████▎                      | 38/200 [01:02<04:40,  1.73s/it]\r\n",
      "DDIM Sampler:  20%|█████▍                      | 39/200 [01:04<04:38,  1.73s/it]\r\n",
      "DDIM Sampler:  20%|█████▌                      | 40/200 [01:06<04:37,  1.73s/it]\r\n",
      "DDIM Sampler:  20%|█████▋                      | 41/200 [01:08<04:36,  1.74s/it]\r\n",
      "DDIM Sampler:  21%|█████▉                      | 42/200 [01:09<04:35,  1.74s/it]\r\n",
      "DDIM Sampler:  22%|██████▏                     | 44/200 [01:13<04:33,  1.75s/it]\r\n",
      "DDIM Sampler:  22%|██████▎                     | 45/200 [01:15<04:32,  1.76s/it]\r\n",
      "DDIM Sampler:  23%|██████▍                     | 46/200 [01:17<04:32,  1.77s/it]\r\n",
      "DDIM Sampler:  24%|██████▌                     | 47/200 [01:18<04:31,  1.78s/it]\r\n",
      "DDIM Sampler:  24%|██████▋                     | 48/200 [01:20<04:31,  1.78s/it]\r\n",
      "DDIM Sampler:  24%|██████▊                     | 49/200 [01:22<04:30,  1.79s/it]\r\n",
      "DDIM Sampler:  25%|███████                     | 50/200 [01:24<04:28,  1.79s/it]\r\n",
      "DDIM Sampler:  26%|███████▏                    | 51/200 [01:26<04:27,  1.79s/it]\r\n",
      "DDIM Sampler:  26%|███████▍                    | 53/200 [01:29<04:25,  1.80s/it]\r\n",
      "DDIM Sampler:  27%|███████▌                    | 54/200 [01:31<04:24,  1.81s/it]\r\n",
      "DDIM Sampler:  28%|███████▋                    | 55/200 [01:33<04:23,  1.82s/it]\r\n",
      "DDIM Sampler:  28%|███████▊                    | 56/200 [01:35<04:22,  1.82s/it]\r\n",
      "DDIM Sampler:  28%|███████▉                    | 57/200 [01:37<04:22,  1.83s/it]\r\n",
      "DDIM Sampler:  29%|████████                    | 58/200 [01:38<04:22,  1.85s/it]\r\n",
      "DDIM Sampler:  30%|████████▎                   | 59/200 [01:40<04:21,  1.86s/it]\r\n",
      "DDIM Sampler:  30%|████████▍                   | 60/200 [01:42<04:21,  1.86s/it]\r\n",
      "DDIM Sampler:  31%|████████▋                   | 62/200 [01:46<04:19,  1.88s/it]\r\n",
      "DDIM Sampler:  32%|████████▊                   | 63/200 [01:48<04:19,  1.90s/it]\r\n",
      "DDIM Sampler:  32%|████████▉                   | 64/200 [01:50<04:18,  1.90s/it]\r\n",
      "DDIM Sampler:  32%|█████████                   | 65/200 [01:52<04:18,  1.92s/it]\r\n",
      "DDIM Sampler:  33%|█████████▏                  | 66/200 [01:54<04:17,  1.92s/it]\r\n",
      "DDIM Sampler:  34%|█████████▍                  | 67/200 [01:56<04:17,  1.94s/it]\r\n",
      "DDIM Sampler:  34%|█████████▌                  | 68/200 [01:58<04:17,  1.95s/it]\r\n",
      "DDIM Sampler:  35%|█████████▊                  | 70/200 [02:02<04:16,  1.98s/it]\r\n",
      "DDIM Sampler:  36%|█████████▉                  | 71/200 [02:04<04:17,  1.99s/it]\r\n",
      "DDIM Sampler:  36%|██████████                  | 72/200 [02:06<04:16,  2.00s/it]\r\n",
      "DDIM Sampler:  36%|██████████▏                 | 73/200 [02:08<04:15,  2.01s/it]\r\n",
      "DDIM Sampler:  37%|██████████▎                 | 74/200 [02:10<04:14,  2.02s/it]\r\n",
      "DDIM Sampler:  38%|██████████▌                 | 75/200 [02:12<04:14,  2.04s/it]\r\n",
      "DDIM Sampler:  38%|██████████▊                 | 77/200 [02:16<04:13,  2.06s/it]\r\n",
      "DDIM Sampler:  39%|██████████▉                 | 78/200 [02:18<04:11,  2.07s/it]\r\n",
      "DDIM Sampler:  40%|███████████                 | 79/200 [02:20<04:09,  2.06s/it]\r\n",
      "DDIM Sampler:  40%|███████████▏                | 80/200 [02:22<04:07,  2.06s/it]\r\n",
      "DDIM Sampler:  40%|███████████▎                | 81/200 [02:24<04:04,  2.05s/it]\r\n",
      "DDIM Sampler:  42%|███████████▌                | 83/200 [02:28<03:57,  2.03s/it]\r\n",
      "DDIM Sampler:  42%|███████████▊                | 84/200 [02:30<03:54,  2.02s/it]\r\n",
      "DDIM Sampler:  42%|███████████▉                | 85/200 [02:32<03:51,  2.01s/it]\r\n",
      "DDIM Sampler:  43%|████████████                | 86/200 [02:34<03:48,  2.00s/it]\r\n",
      "DDIM Sampler:  44%|████████████▎               | 88/200 [02:38<03:42,  1.99s/it]\r\n",
      "DDIM Sampler:  44%|████████████▍               | 89/200 [02:40<03:39,  1.97s/it]\r\n",
      "DDIM Sampler:  45%|████████████▌               | 90/200 [02:42<03:36,  1.97s/it]\r\n",
      "DDIM Sampler:  46%|████████████▋               | 91/200 [02:44<03:33,  1.96s/it]\r\n",
      "DDIM Sampler:  46%|████████████▉               | 92/200 [02:46<03:31,  1.95s/it]\r\n",
      "DDIM Sampler:  47%|█████████████▏              | 94/200 [02:50<03:26,  1.95s/it]\r\n",
      "DDIM Sampler:  48%|█████████████▎              | 95/200 [02:52<03:23,  1.94s/it]\r\n",
      "DDIM Sampler:  48%|█████████████▍              | 96/200 [02:54<03:21,  1.94s/it]\r\n",
      "DDIM Sampler:  48%|█████████████▌              | 97/200 [02:56<03:18,  1.93s/it]\r\n",
      "DDIM Sampler:  49%|█████████████▋              | 98/200 [02:58<03:16,  1.93s/it]\r\n",
      "DDIM Sampler:  50%|█████████████▌             | 100/200 [03:01<03:12,  1.93s/it]\r\n",
      "DDIM Sampler:  50%|█████████████▋             | 101/200 [03:03<03:10,  1.93s/it]\r\n",
      "DDIM Sampler:  51%|█████████████▊             | 102/200 [03:05<03:09,  1.93s/it]\r\n",
      "DDIM Sampler:  52%|█████████████▉             | 103/200 [03:07<03:07,  1.93s/it]\r\n",
      "DDIM Sampler:  52%|██████████████             | 104/200 [03:09<03:05,  1.94s/it]\r\n",
      "DDIM Sampler:  53%|██████████████▎            | 106/200 [03:13<03:02,  1.94s/it]\r\n",
      "DDIM Sampler:  54%|██████████████▍            | 107/200 [03:15<03:00,  1.95s/it]\r\n",
      "DDIM Sampler:  54%|██████████████▌            | 108/200 [03:17<02:59,  1.95s/it]\r\n",
      "DDIM Sampler:  55%|██████████████▋            | 109/200 [03:19<02:58,  1.96s/it]\r\n",
      "DDIM Sampler:  55%|██████████████▊            | 110/200 [03:21<02:56,  1.97s/it]\r\n",
      "DDIM Sampler:  56%|██████████████▉            | 111/200 [03:23<02:55,  1.97s/it]\r\n",
      "DDIM Sampler:  56%|███████████████▎           | 113/200 [03:27<02:51,  1.98s/it]\r\n",
      "DDIM Sampler:  57%|███████████████▍           | 114/200 [03:29<02:50,  1.98s/it]\r\n",
      "DDIM Sampler:  57%|███████████████▌           | 115/200 [03:31<02:48,  1.99s/it]\r\n",
      "DDIM Sampler:  58%|███████████████▋           | 116/200 [03:33<02:47,  1.99s/it]\r\n",
      "DDIM Sampler:  58%|███████████████▊           | 117/200 [03:35<02:45,  2.00s/it]\r\n",
      "DDIM Sampler:  60%|████████████████           | 119/200 [03:39<02:42,  2.00s/it]\r\n",
      "DDIM Sampler:  60%|████████████████▏          | 120/200 [03:41<02:40,  2.00s/it]\r\n",
      "DDIM Sampler:  60%|████████████████▎          | 121/200 [03:43<02:38,  2.00s/it]\r\n",
      "DDIM Sampler:  61%|████████████████▍          | 122/200 [03:45<02:36,  2.01s/it]\r\n",
      "DDIM Sampler:  62%|████████████████▌          | 123/200 [03:47<02:34,  2.01s/it]\r\n",
      "DDIM Sampler:  62%|████████████████▉          | 125/200 [03:51<02:29,  2.00s/it]\r\n",
      "DDIM Sampler:  63%|█████████████████          | 126/200 [03:53<02:27,  2.00s/it]\r\n",
      "DDIM Sampler:  64%|█████████████████▏         | 127/200 [03:55<02:25,  2.00s/it]\r\n",
      "DDIM Sampler:  64%|█████████████████▎         | 128/200 [03:57<02:23,  1.99s/it]\r\n",
      "DDIM Sampler:  64%|█████████████████▍         | 129/200 [03:59<02:21,  1.99s/it]\r\n",
      "DDIM Sampler:  65%|█████████████████▌         | 130/200 [04:01<02:19,  1.99s/it]\r\n",
      "DDIM Sampler:  66%|█████████████████▊         | 132/200 [04:05<02:14,  1.98s/it]\r\n",
      "DDIM Sampler:  66%|█████████████████▉         | 133/200 [04:07<02:12,  1.98s/it]\r\n",
      "DDIM Sampler:  67%|██████████████████         | 134/200 [04:09<02:10,  1.98s/it]\r\n",
      "DDIM Sampler:  68%|██████████████████▏        | 135/200 [04:11<02:08,  1.98s/it]\r\n",
      "DDIM Sampler:  68%|██████████████████▎        | 136/200 [04:13<02:06,  1.98s/it]\r\n",
      "DDIM Sampler:  69%|██████████████████▋        | 138/200 [04:17<02:02,  1.97s/it]\r\n",
      "DDIM Sampler:  70%|██████████████████▊        | 139/200 [04:19<01:59,  1.97s/it]\r\n",
      "DDIM Sampler:  70%|██████████████████▉        | 140/200 [04:21<01:57,  1.97s/it]\r\n",
      "DDIM Sampler:  70%|███████████████████        | 141/200 [04:22<01:55,  1.96s/it]\r\n",
      "DDIM Sampler:  71%|███████████████████▏       | 142/200 [04:24<01:53,  1.96s/it]\r\n",
      "DDIM Sampler:  72%|███████████████████▎       | 143/200 [04:26<01:52,  1.97s/it]\r\n",
      "DDIM Sampler:  72%|███████████████████▌       | 145/200 [04:30<01:48,  1.97s/it]\r\n",
      "DDIM Sampler:  73%|███████████████████▋       | 146/200 [04:32<01:46,  1.97s/it]\r\n",
      "DDIM Sampler:  74%|███████████████████▊       | 147/200 [04:34<01:44,  1.97s/it]\r\n",
      "DDIM Sampler:  74%|███████████████████▉       | 148/200 [04:36<01:42,  1.97s/it]\r\n",
      "DDIM Sampler:  74%|████████████████████       | 149/200 [04:38<01:40,  1.97s/it]\r\n",
      "DDIM Sampler:  76%|████████████████████▍      | 151/200 [04:42<01:36,  1.98s/it]\r\n",
      "DDIM Sampler:  76%|████████████████████▌      | 152/200 [04:44<01:34,  1.97s/it]\r\n",
      "DDIM Sampler:  76%|████████████████████▋      | 153/200 [04:46<01:32,  1.97s/it]\r\n",
      "DDIM Sampler:  77%|████████████████████▊      | 154/200 [04:48<01:30,  1.98s/it]\r\n",
      "DDIM Sampler:  78%|████████████████████▉      | 155/200 [04:50<01:29,  1.98s/it]\r\n",
      "DDIM Sampler:  78%|█████████████████████▏     | 157/200 [04:54<01:25,  1.98s/it]\r\n",
      "DDIM Sampler:  79%|█████████████████████▎     | 158/200 [04:56<01:22,  1.98s/it]\r\n",
      "DDIM Sampler:  80%|█████████████████████▍     | 159/200 [04:58<01:21,  1.98s/it]\r\n",
      "DDIM Sampler:  80%|█████████████████████▌     | 160/200 [05:00<01:19,  1.98s/it]\r\n",
      "DDIM Sampler:  80%|█████████████████████▋     | 161/200 [05:02<01:17,  1.98s/it]\r\n",
      "DDIM Sampler:  82%|██████████████████████     | 163/200 [05:06<01:13,  1.98s/it]\r\n",
      "DDIM Sampler:  82%|██████████████████████▏    | 164/200 [05:08<01:11,  1.98s/it]\r\n",
      "DDIM Sampler:  82%|██████████████████████▎    | 165/200 [05:10<01:09,  1.98s/it]\r\n",
      "DDIM Sampler:  83%|██████████████████████▍    | 166/200 [05:12<01:07,  1.98s/it]\r\n",
      "DDIM Sampler:  84%|██████████████████████▋    | 168/200 [05:16<01:03,  1.98s/it]\r\n",
      "DDIM Sampler:  84%|██████████████████████▊    | 169/200 [05:18<01:01,  1.98s/it]\r\n",
      "DDIM Sampler:  85%|██████████████████████▉    | 170/200 [05:20<00:59,  1.98s/it]\r\n",
      "DDIM Sampler:  86%|███████████████████████    | 171/200 [05:22<00:57,  1.98s/it]\r\n",
      "DDIM Sampler:  86%|███████████████████████▏   | 172/200 [05:24<00:55,  1.98s/it]\r\n",
      "DDIM Sampler:  87%|███████████████████████▍   | 174/200 [05:28<00:51,  1.98s/it]\r\n",
      "DDIM Sampler:  88%|███████████████████████▋   | 175/200 [05:30<00:49,  1.98s/it]\r\n",
      "DDIM Sampler:  88%|███████████████████████▊   | 176/200 [05:32<00:47,  1.98s/it]\r\n",
      "DDIM Sampler:  88%|███████████████████████▉   | 177/200 [05:34<00:45,  1.98s/it]\r\n",
      "DDIM Sampler:  89%|████████████████████████   | 178/200 [05:36<00:43,  1.98s/it]\r\n",
      "DDIM Sampler:  90%|████████████████████████▎  | 180/200 [05:40<00:39,  1.98s/it]\r\n",
      "DDIM Sampler:  90%|████████████████████████▍  | 181/200 [05:42<00:37,  1.98s/it]\r\n",
      "DDIM Sampler:  91%|████████████████████████▌  | 182/200 [05:44<00:35,  1.98s/it]\r\n",
      "DDIM Sampler:  92%|████████████████████████▋  | 183/200 [05:46<00:33,  1.98s/it]\r\n",
      "DDIM Sampler:  92%|████████████████████████▊  | 184/200 [05:48<00:31,  1.98s/it]\r\n",
      "DDIM Sampler:  92%|████████████████████████▉  | 185/200 [05:50<00:29,  1.98s/it]\r\n",
      "DDIM Sampler:  94%|█████████████████████████▏ | 187/200 [05:54<00:25,  1.98s/it]\r\n",
      "DDIM Sampler:  94%|█████████████████████████▍ | 188/200 [05:56<00:23,  1.98s/it]\r\n",
      "DDIM Sampler:  94%|█████████████████████████▌ | 189/200 [05:57<00:21,  1.98s/it]\r\n",
      "DDIM Sampler:  95%|█████████████████████████▋ | 190/200 [05:59<00:19,  1.98s/it]\r\n",
      "DDIM Sampler:  96%|█████████████████████████▊ | 191/200 [06:01<00:17,  1.98s/it]\r\n",
      "DDIM Sampler:  96%|██████████████████████████ | 193/200 [06:05<00:13,  1.98s/it]\r\n",
      "DDIM Sampler:  97%|██████████████████████████▏| 194/200 [06:07<00:11,  1.98s/it]\r\n",
      "DDIM Sampler:  98%|██████████████████████████▎| 195/200 [06:09<00:09,  1.98s/it]\r\n",
      "DDIM Sampler:  98%|██████████████████████████▍| 196/200 [06:11<00:07,  1.98s/it]\r\n",
      "DDIM Sampler:  98%|██████████████████████████▌| 197/200 [06:13<00:05,  1.98s/it]\r\n",
      "DDIM Sampler:  99%|██████████████████████████▋| 198/200 [06:15<00:03,  1.98s/it]\r\n",
      "DDIM Sampler: 100%|███████████████████████████| 200/200 [06:19<00:00,  1.90s/it]\r\n",
      "\r\n",
      "DDIM Sampler:  86%|███████████████████████▎   | 173/200 [06:20<01:03,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  87%|███████████████████████▍   | 174/200 [06:22<01:01,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  88%|███████████████████████▋   | 175/200 [06:24<00:58,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  88%|███████████████████████▊   | 176/200 [06:27<00:56,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  88%|███████████████████████▉   | 177/200 [06:29<00:53,  2.35s/it]\u001b[AINFO: clap model calculate the audio embedding as condition\r\n",
      "Similarity between generated audio and text tensor([-0.0973, -0.2498, -0.1122, -0.1094, -0.0607, -0.0428, -0.1433, -0.0116,\r\n",
      "        -0.0882, -0.1108, -0.1048, -0.1186,  0.1634, -0.0109, -0.1541, -0.0738,\r\n",
      "        -0.0750, -0.2255, -0.1379, -0.1123, -0.0801, -0.0350, -0.1030,  0.1763],\r\n",
      "       device='cuda:1')\r\n",
      "Choose the following indexes: [16, 9, 10, 3, 12, 13, 22, 23]\r\n",
      "Plotting: Restored training weights\r\n",
      "Change back the embedding mode to text text\r\n",
      "\r\n",
      "DDIM Sampler:  89%|████████████████████████   | 178/200 [06:31<00:51,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  90%|████████████████████████▏  | 179/200 [06:34<00:49,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  90%|████████████████████████▎  | 180/200 [06:36<00:46,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  90%|████████████████████████▍  | 181/200 [06:38<00:44,  2.34s/it]\u001b[A\r\n",
      "DDIM Sampler:  91%|████████████████████████▌  | 182/200 [06:41<00:42,  2.34s/it]\u001b[A\r\n",
      "DDIM Sampler:  92%|████████████████████████▋  | 183/200 [06:43<00:39,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  92%|████████████████████████▊  | 184/200 [06:45<00:37,  2.34s/it]\u001b[A\r\n",
      "DDIM Sampler:  92%|████████████████████████▉  | 185/200 [06:48<00:35,  2.35s/it]\u001b[A\r\n",
      "DDIM Sampler:  93%|█████████████████████████  | 186/200 [06:50<00:32,  2.36s/it]\u001b[A\r\n",
      "DDIM Sampler:  94%|█████████████████████████▏ | 187/200 [06:53<00:30,  2.36s/it]\u001b[A\r\n",
      "DDIM Sampler:  94%|█████████████████████████▍ | 188/200 [06:55<00:28,  2.37s/it]\u001b[A\r\n",
      "DDIM Sampler:  94%|█████████████████████████▌ | 189/200 [06:57<00:26,  2.38s/it]\u001b[A\r\n",
      "DDIM Sampler:  95%|█████████████████████████▋ | 190/200 [07:00<00:23,  2.38s/it]\u001b[A\r\n",
      "DDIM Sampler:  96%|█████████████████████████▊ | 191/200 [07:02<00:21,  2.38s/it]\u001b[A\r\n",
      "DDIM Sampler:  96%|█████████████████████████▉ | 192/200 [07:05<00:19,  2.39s/it]\u001b[A\r\n",
      "DDIM Sampler:  96%|██████████████████████████ | 193/200 [07:07<00:16,  2.40s/it]\u001b[A\r\n",
      "DDIM Sampler:  97%|██████████████████████████▏| 194/200 [07:09<00:14,  2.40s/it]\u001b[A\r\n",
      "DDIM Sampler:  98%|██████████████████████████▎| 195/200 [07:12<00:12,  2.41s/it]\u001b[A\r\n",
      "DDIM Sampler:  98%|██████████████████████████▍| 196/200 [07:14<00:09,  2.41s/it]\u001b[A\r\n",
      "DDIM Sampler:  98%|██████████████████████████▌| 197/200 [07:17<00:07,  2.42s/it]\u001b[A\r\n",
      "DDIM Sampler:  99%|██████████████████████████▋| 198/200 [07:19<00:04,  2.42s/it]\u001b[A\r\n",
      "DDIM Sampler: 100%|██████████████████████████▊| 199/200 [07:21<00:02,  2.41s/it]\u001b[A\r\n",
      "DDIM Sampler: 100%|███████████████████████████| 200/200 [07:24<00:00,  2.22s/it]\r\n",
      "INFO: clap model calculate the audio embedding as condition\r\n",
      "Similarity between generated audio and text tensor([ 0.0874, -0.1206, -0.0569, -0.0929, -0.0654, -0.1023, -0.1220, -0.0351,\r\n",
      "         0.1090, -0.1442, -0.1141, -0.0639,  0.1634, -0.0795, -0.1117, -0.0426,\r\n",
      "         0.1059, -0.0985, -0.1198, -0.0694, -0.0446, -0.0848, -0.1272,  0.1763],\r\n",
      "       device='cuda:0')\r\n",
      "Choose the following indexes: [8, 17, 2, 11, 12, 13, 14, 23]\r\n",
      "Plotting: Restored training weights\r\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 1/1 [07:40<00:00,  0.00it/s]Change back the embedding mode to text text\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(_create_warning_msg(\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\r\n",
      "  warnings.warn(_create_warning_msg(\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Epoch 0:   0%|                                        | 0/12376 [00:00<?, ?it/s]Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "### USING STD-RESCALING ###\r\n",
      "setting self.scale_factor to 0.5096375346183777\r\n",
      "### USING STD-RESCALING ###\r\n",
      "Warming up learning rate start with 0.0001\r\n",
      "Warming up learning rate start with 0.0001\r\n",
      "[rank0]:[W1122 21:10:44.775468807 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[rank1]:[W1122 21:10:44.861572511 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\r\n",
      "grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]\r\n",
      "bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /usr/local/src/pytorch/torch/csrc/distributed/c10d/reducer.cpp:327.)\r\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\r\n",
      "grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]\r\n",
      "bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at /usr/local/src/pytorch/torch/csrc/distributed/c10d/reducer.cpp:327.)\r\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "Epoch 0:  34%|▎| 4236/12376 [1:01:06<1:57:25,  1.16it/s, v_num=nvpd, train/loss_Error encounter during audio feature extraction:  Failed to decode audio. ./data/dataset/audioset/zip_audios/unbalanced_train_segments/unbalanced_train_segments_part25/YWudGD6ZHRoY.wav\r\n",
      "Epoch 0: 100%|█| 12376/12376 [2:58:33<00:00,  1.16it/s, v_num=nvpd, train/loss_s/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss_simple', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\r\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss_vlb', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\r\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\r\n",
      "Epoch 0: 100%|█| 12376/12376 [2:58:33<00:00,  1.16it/s, v_num=nvpd, train/loss_sLog directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Epoch 1:   0%| | 0/12376 [00:00<?, ?it/s, v_num=nvpd, train/loss_simple_step=0.0Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Epoch 1:  29%|▎| 3636/12376 [52:41<2:06:39,  1.15it/s, v_num=nvpd, train/loss_siError encounter during audio feature extraction:  Failed to decode audio. ./data/dataset/audioset/zip_audios/unbalanced_train_segments/unbalanced_train_segments_part25/YWudGD6ZHRoY.wav\r\n",
      "Epoch 1: 100%|█| 12376/12376 [2:59:27<00:00,  1.15it/s, v_num=nvpd, train/loss_sLog directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Epoch 2:   0%| | 0/12376 [00:00<?, ?it/s, v_num=nvpd, train/loss_simple_step=0.1Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Epoch 2:  28%|▎| 3447/12376 [49:57<2:09:23,  1.15it/s, v_num=nvpd, train/loss_siError encounter during audio feature extraction:  Failed to decode audio. ./data/dataset/audioset/zip_audios/unbalanced_train_segments/unbalanced_train_segments_part25/YWudGD6ZHRoY.wav\r\n",
      "Epoch 3:   0%| | 0/12376 [00:00<?, ?it/s, v_num=nvpd, train/loss_simple_step=0.0Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Log directory:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original\r\n",
      "Epoch 3:  64%|▋| 7872/12376 [1:54:28<1:05:29,  1.15it/s, v_num=nvpd, train/loss_\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Disable wandb\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "import gzip\n",
    "\n",
    "original_file = '/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/clap/open_clip/bpe_simple_vocab_16e6.txt'\n",
    "target_file = '/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/clap/open_clip/bpe_simple_vocab_16e6.txt.gz'\n",
    "\n",
    "with open(original_file, 'rb') as f_in, gzip.open(target_file, 'wb') as f_out:\n",
    "    f_out.writelines(f_in)\n",
    "\n",
    "# Run the training script\n",
    "!python audioldm_train/train/latent_diffusion.py -c audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original.yaml\n",
    "# !python3 audioldm_train/train/autoencoder.py -c audioldm_train/config/2023_11_13_vae_autoencoder/16k_64.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fbcb7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T08:02:32.823914Z",
     "iopub.status.busy": "2024-11-23T08:02:32.823556Z",
     "iopub.status.idle": "2024-11-23T08:02:32.828280Z",
     "shell.execute_reply": "2024-11-23T08:02:32.827475Z"
    },
    "papermill": {
     "duration": 4.210633,
     "end_time": "2024-11-23T08:02:32.830001",
     "exception": false,
     "start_time": "2024-11-23T08:02:28.619368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 audioldm_train/infer.py --config_yaml audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original.yaml --list_inference tests/captionlist/inference_test_with_filename.lst --reload_from_ckpt log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/checkpoints/checkpoint-fad-133.00-global_step=49.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aab4e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T08:02:41.068966Z",
     "iopub.status.busy": "2024-11-23T08:02:41.068648Z",
     "iopub.status.idle": "2024-11-23T08:02:41.075243Z",
     "shell.execute_reply": "2024-11-23T08:02:41.074492Z"
    },
    "papermill": {
     "duration": 4.192583,
     "end_time": "2024-11-23T08:02:41.076913",
     "exception": false,
     "start_time": "2024-11-23T08:02:36.884330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['audioldm_original', 'audioldm_crossattn_flant5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tmp_sub_dir)\n",
    "os.listdir(tmp_sub_dir + '/log/latent_diffusion/2023_08_23_reproduce_audioldm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43554009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T08:02:49.192386Z",
     "iopub.status.busy": "2024-11-23T08:02:49.192019Z",
     "iopub.status.idle": "2024-11-23T08:02:50.346495Z",
     "shell.execute_reply": "2024-11-23T08:02:50.345653Z"
    },
    "papermill": {
     "duration": 5.226204,
     "end_time": "2024-11-23T08:02:50.348449",
     "exception": false,
     "start_time": "2024-11-23T08:02:45.122245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\r\n",
      "drwxr-xr-x 4 root root 4096 Nov 22 21:02 .\r\n",
      "drwxr-xr-x 4 root root 4096 Nov 22 21:02 ..\r\n",
      "-rw-r--r-- 1 root root 4211 Nov 22 21:02 audioldm_original.yaml\r\n",
      "drwxr-xr-x 2 root root 4096 Nov 23 08:02 checkpoints\r\n",
      "drwxr-xr-x 2 root root 4096 Nov 22 21:10 infer_11-22-21:02_cfg_scale_3.5_ddim_200_n_cand_3\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la /kaggle/tmp/AudioLDM-training-finetuning/log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cc32c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T08:02:58.416632Z",
     "iopub.status.busy": "2024-11-23T08:02:58.415748Z",
     "iopub.status.idle": "2024-11-23T08:12:01.815293Z",
     "shell.execute_reply": "2024-11-23T08:12:01.814373Z"
    },
    "papermill": {
     "duration": 547.429297,
     "end_time": "2024-11-23T08:12:01.818715",
     "exception": false,
     "start_time": "2024-11-23T08:02:54.389418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning/log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/checkpoint-fad-133.00-global_step=44999.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:145: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/clap/open_clip/factory.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/taming/modules/losses/lpips.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/model_util.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path + \".ckpt\")\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/modules/latent_encoder/autoencoder.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.reload_from_ckpt)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/infer.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_from_checkpoint)\n",
      "/kaggle/tmp/AudioLDM-training-finetuning/audioldm_train/utilities/data/dataset.py:455: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = librosa_mel_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED EVERYTHING TO 0\n",
      "Add-ons: []\n",
      "Dataset initialize finished\n",
      "Load checkpoint from path: ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/checkpoints\n",
      "Resume from checkpoint ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/checkpoints/checkpoint-fad-133.00-global_step=44999.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "+ Use extra condition on UNet channel using Film. Extra condition dimension is 512. \n",
      "DiffusionWrapper has 185.04 M params.\n",
      "Keeping EMAs of 692.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Removing weight norm...\n",
      "Initial learning rate 1e-05\n",
      "--> Reload weight of autoencoder from data/checkpoints/vae_mel_16k_64bins.ckpt\n",
      "Waveform inference save path:  ./log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/infer_11-23-08:04_cfg_scale_3.5_ddim_200_n_cand_3\n",
      "Plotting: Switched to EMA weights\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile01.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:10<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of UNet input is torch.Size([3, 8, 256, 16])\n",
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([ 0.1145,  0.0122, -0.0720], device='cuda:0')\n",
      "Choose the following indexes: [0]\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile02.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:14<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([-0.0208,  0.1667,  0.0994], device='cuda:0')\n",
      "Choose the following indexes: [1]\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile03.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:14<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([-0.2383, -0.0535,  0.2408], device='cuda:0')\n",
      "Choose the following indexes: [2]\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile04.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:14<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([ 0.1639, -0.1586,  0.2934], device='cuda:0')\n",
      "Choose the following indexes: [2]\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile05.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:14<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([-0.0103,  0.1107, -0.1062], device='cuda:0')\n",
      "Choose the following indexes: [1]\n",
      "Non-fatal Warning [dataset.py]: The wav path \" audiofile06.wav \" is not find in the metadata. Use empty waveform instead. This is normal in the inference process.\n",
      "Warning: CLAP model normally should use text for evaluation\n",
      "Use ddim sampler\n",
      "Data shape for DDIM sampling is (3, 8, 256, 16), eta 1.0\n",
      "Running DDIM Sampling with 200 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [01:13<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: clap model calculate the audio embedding as condition\n",
      "Similarity between generated audio and text tensor([0.1712, 0.1284, 0.1977], device='cuda:0')\n",
      "Choose the following indexes: [2]\n",
      "Plotting: Restored training weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = os.listdir('/kaggle/tmp/AudioLDM-training-finetuning/log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/checkpoints')[0]\n",
    "ckpt_path = '/kaggle/tmp/AudioLDM-training-finetuning/log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original/' + ckpt_name\n",
    "print(ckpt_path)\n",
    "os.system(f'python3 audioldm_train/infer.py --config_yaml audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_original.yaml --list_inference tests/captionlist/inference_test_with_filename.lst --reload_from_ckpt {ckpt_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa0907bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T08:12:09.986931Z",
     "iopub.status.busy": "2024-11-23T08:12:09.986608Z",
     "iopub.status.idle": "2024-11-23T08:12:14.850329Z",
     "shell.execute_reply": "2024-11-23T08:12:14.849149Z"
    },
    "papermill": {
     "duration": 8.884336,
     "end_time": "2024-11-23T08:12:14.853434",
     "exception": false,
     "start_time": "2024-11-23T08:12:05.969098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copytree('/kaggle/tmp/AudioLDM-training-finetuning/log/latent_diffusion/2023_08_23_reproduce_audioldm/audioldm_original', '/kaggle/working/', dirs_exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5954834,
     "sourceId": 9730723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6121684,
     "sourceId": 9953798,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6131766,
     "sourceId": 9967575,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41395.007055,
   "end_time": "2024-11-23T08:12:20.014908",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-22T20:42:25.007853",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
